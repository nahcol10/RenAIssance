{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb438af6-d79d-4fde-b060-b5553e957c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for psyduck: "
     ]
    }
   ],
   "source": [
    "# Installing Necessary Packages\n",
    "!sudo apt-get install-y libjpeg-dev zlib1g-dev graphviz \n",
    "%pip install scikit-image python-docx pymupdf opencv-python torch torchvision tens\n",
    "\n",
    "# Importing Necessary Modules\n",
    "import fitz\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "from docx import Document\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from IPython.display import clear_output as cls\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow.data as tfd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Downloading the Dataset on Colab/Local Notebook\n",
    "!pip install gdown\n",
    "!gdown 'http://drive.google.com/uc?id=10NX_UbV2HMbPEO2fvKYAIXOOOec0g38g'  # Virtuosa Dataset\n",
    "!gdown 'http://drive.google.com/uc?id=1x6FS3z4WhsHS7s38a2oSH8JnLQ_u_f21'  # Perfecto Dataset\n",
    "!gdown 'http://drive.google.com/uc?id=1r7TjJ9RjNZHxAzKhd4uOaRWanrQXIqw8'  # utils.py\n",
    "\n",
    "# Image Augmentation Functions\n",
    "from utils import rotation_aug\n",
    "training_data = './training_data'\n",
    "rotation_aug(training_data)\n",
    "print(\"Image augmentation by Rotation completed.\")\n",
    "\n",
    "from utils import gaussian_noise_aug\n",
    "gaussian_noise_aug(training_data)\n",
    "print(\"Image augmentation by Gaussian Noise completed.\")\n",
    "\n",
    "# Creating CSV Files\n",
    "from utils import create_csv_from_folder\n",
    "training_data = './training_data'\n",
    "train_csv_path = './training_data.csv'\n",
    "create_csv_from_folder(training_data, train_csv_path)\n",
    "test_data1 = \"./testing_data1\"\n",
    "test_csv_path1 = './testing_data1.csv'\n",
    "create_csv_from_folder(test_data1, test_csv_path1)\n",
    "\n",
    "# Splitting Train and Test Dataset\n",
    "df = pd.read_csv('training_data.csv')\n",
    "df.shape\n",
    "df['IDENTITY'] = df['IDENTITY'].apply(lambda x: x.split('_')[0]).apply(lambda x: str(x))\n",
    "TRAIN_SIZE = int(df.shape[0] * 0.8)\n",
    "df_train = df.iloc[:TRAIN_SIZE]\n",
    "df_valid = df.iloc[TRAIN_SIZE + 1:]\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_valid.to_csv('valid.csv', index=False)\n",
    "\n",
    "# Configuration Parameters\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "IMAGE_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "MODEL_NAME = 'SpanishOCR'\n",
    "CALLBACKS = [\n",
    "    callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    callbacks.ModelCheckpoint(filepath=MODEL_NAME + \".h5\", save_best_only=True)\n",
    "]\n",
    "LEARNING_RATE = 1e-3\n",
    "np.random.seed(2569)\n",
    "tf.random.set_seed(2569)\n",
    "train_csv_path = './train.csv'\n",
    "valid_csv_path = './valid.csv'\n",
    "test_csv_path = './testing_data1.csv'\n",
    "train_image_dir = './training_data'\n",
    "valid_image_dir = './training_data'\n",
    "test_image_dir = './testing_data1'\n",
    "AUTOTUNE = tfd.AUTOTUNE\n",
    "\n",
    "# Extracting Unique Characters\n",
    "labels = [str(word) for word in df['IDENTITY'].to_numpy()]\n",
    "unique_chars = set(char for word in labels for char in word)\n",
    "n_classes = len(unique_chars)\n",
    "print(f\"Total number of unique characters: {n_classes}\")\n",
    "print(f\"Unique Characters:\\n{unique_chars}\")\n",
    "MAX_LABEL_LENGTH = max(map(len, labels))\n",
    "print(f\"Maximum length of a label: {MAX_LABEL_LENGTH}\")\n",
    "\n",
    "# Mapping Characters to Numeric Values\n",
    "char_to_num = layers.StringLookup(\n",
    "    vocabulary=list(unique_chars),\n",
    "    mask_token=None\n",
    ")\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(),\n",
    "    mask_token=None,\n",
    "    invert=True\n",
    ")\n",
    "\n",
    "# Image Loading and Preprocessing\n",
    "def load_image(image_path: str):\n",
    "    '''Loads and preprocesses images.'''\n",
    "    image = tf.io.read_file(image_path)\n",
    "    decoded_image = tf.image.decode_jpeg(contents=image, channels=1)\n",
    "    cnvt_image = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "    resized_image = tf.image.resize(images=cnvt_image, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.transpose(resized_image, perm=[1, 0, 2])\n",
    "    return image\n",
    "\n",
    "# Encoding Single Sample\n",
    "def encode_single_sample(image_path: str, label: str):\n",
    "    '''Encodes a single sample with image and label.'''\n",
    "    image = load_image(image_path)\n",
    "    chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "    vecs = char_to_num(chars)\n",
    "    pad_size = MAX_LABEL_LENGTH - tf.shape(vecs)[0]\n",
    "    vecs = tf.pad(vecs, paddings=[[0, pad_size]], constant_values=n_classes)\n",
    "    return {'image': image, 'label': vecs}\n",
    "\n",
    "# Creating TensorFlow Datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(train_csv['FILENAME'].to_list()), np.array(train_csv['IDENTITY'].to_list()))\n",
    ").shuffle(train_size).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(valid_csv['FILENAME'].to_list()), np.array(valid_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(test_csv['FILENAME'].to_list()), np.array(test_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Resizing Images in Folder\n",
    "def resize_images_in_folder(input_folder, new_size=(200, 50)):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
    "            resized_img = img.resize(new_size)\n",
    "            output_filename = os.path.splitext(filename)[0] + '.png'\n",
    "            resized_img.save(os.path.join(input_folder, output_filename))\n",
    "\n",
    "training_image_dir = \"./training_data\"\n",
    "resize_images_in_folder(training_image_dir)\n",
    "test_image_dir = \"./testing_data1\"\n",
    "resize_images_in_folder(test_image_dir)\n",
    "\n",
    "# Displaying Images\n",
    "def show_images(data, GRID=[4, 4], FIGSIZE=(25, 8), cmap='binary_r', model=None, decode_pred=None):\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    n_rows, n_cols = GRID\n",
    "    data = next(iter(data))\n",
    "    images, labels = data['image'], data['label']\n",
    "    for index, (image, label) in enumerate(zip(images, labels)):\n",
    "        text_label = num_to_char(label)\n",
    "        text_label = tf.strings.reduce_join(text_label).numpy().decode('UTF-8')\n",
    "        text_label = text_label.replace(\"[UNK]\", \"\").strip()\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(tf.transpose(image, perm=[1, 0, 2]), cmap=cmap)\n",
    "        plt.axis('off')\n",
    "        if model is not None and decode_pred is not None:\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "            pred = decode_pred(pred)[0]\n",
    "            title = f\"True: {text_label}\\nPred: {pred}\"\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            plt.title(text_label)\n",
    "    plt.show()\n",
    "\n",
    "show_images(data=train_ds, cmap='gray')\n",
    "\n",
    "# Compiling and Training the Model\n",
    "ocr_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6\n",
    ")\n",
    "history = ocr_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        lr_scheduler\n",
    "    ]\n",
    ")\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"CTC Loss Score\")\n",
    "plt.title(\"Learning Curve\", fontsize=15)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Inference Model\n",
    "inference_model = keras.Model(\n",
    "    inputs=ocr_model.input[0],\n",
    "    outputs=ocr_model.get_layer(name=\"dense_1\").output\n",
    ")\n",
    "print(decode_pred(inference_model.predict(test_ds))[:10])  # Convert to test_ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
