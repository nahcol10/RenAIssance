{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZMxirZBNQIH"
   },
   "source": [
    "# Installing necessary packages\n",
    "### This block installs system libraries and Python packages necessary for image processing, document handling, and deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDODJsdzNY3t",
    "outputId": "0d574451-b3d4-4045-a5f7-24c8a3f055b8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (0.24.0)\n",
      "Requirement already satisfied: PyMuPDF in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (1.25.3)\n",
      "Requirement already satisfied: opencv-python in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: torch in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: tensorflow in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: python-docx in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: mmengine in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (0.10.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (2024.6.18)\n",
      "Requirement already satisfied: packaging>=21 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: filelock in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: addict in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from mmengine) (2.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from mmengine) (3.9.4)\n",
      "Requirement already satisfied: pyyaml in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from mmengine) (6.0.2)\n",
      "Requirement already satisfied: rich in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from mmengine) (13.9.4)\n",
      "Requirement already satisfied: yapf in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from mmengine) (0.43.0)\n",
      "Requirement already satisfied: namex in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from matplotlib->mmengine) (6.5.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from rich->mmengine) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from rich->mmengine) (2.15.1)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from yapf->mmengine) (3.10.0)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from yapf->mmengine) (2.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->mmengine) (3.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/psyduck/anaconda3/envs/renaissance/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# !sudo apt-get install -y libjpeg-dev zlib1g-dev graphviz\n",
    "# !pip install scikit-image PyMuPDF opencv-python torch torchvision tensorflow python-docx mmengine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elI3lOO-NZtO"
   },
   "source": [
    "# Importing required modules\n",
    "### This section imports all the necessary Python libraries for data preprocessing, handling, visualization, and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FNy5s-zyOOKw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Pre-processing\n",
    "import fitz\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "from docx import Document\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from IPython.display import clear_output as cls\n",
    "\n",
    "# Data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow.data as tfd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zjB7D6BOO17"
   },
   "source": [
    "# Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PS6eYteOToV",
    "outputId": "eb9b1186-eaa2-4e92-d8d3-fabb1c2dad59"
   },
   "outputs": [],
   "source": [
    "# Downloading the Dataset on collab/local notebook\n",
    "\n",
    "# Virtuosa ( Dataset 1)\n",
    "!gdown 'http://drive.google.com/uc?id=10NX_UbV2HMbPEO2fvKYAIXOOOec0g38g'  # Downloading link for Ancient Text\n",
    "\n",
    "# Perfecto ( Dataset 2)\n",
    "!gdown 'http://drive.google.com/uc?id=1x6FS3z4WhsHS7s38a2oSH8JnLQ_u_f21'  # Downloading link for Ancient Text\n",
    "\n",
    "!gdown 'http://drive.google.com/uc?id=1r7TjJ9RjNZHxAzKhd4uOaRWanrQXIqw8'  # Downloading utils.py from cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1_XnH9QOrbE",
    "outputId": "97d888dc-0580-40a6-c68b-15301e1f1493"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjsuG_mJOwBk"
   },
   "source": [
    "# Pre-processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3g_AanojUG3"
   },
   "source": [
    "### Converting PDF to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiOV4cLmOxFT"
   },
   "outputs": [],
   "source": [
    "from utils import pdf_to_images\n",
    "\n",
    "pdf_path1 = \"./Padilla - Nobleza virtuosa_testExtract.pdf\"  # Path to the PDF file\n",
    "unproc_images_folder_1 = \"./preprocessing/imgsUnProcessed1\"  # Output folder to save the images\n",
    "if not os.path.exists(unproc_images_folder_1):\n",
    "    os.makedirs(unproc_images_folder_1)\n",
    "pdf_to_images(pdf_path1, unproc_images_folder_1)\n",
    "\n",
    "\n",
    "pdf_path2 = \"./Padilla - 2 Noble perfecto_Extract.pdf\"  # Path to the PDF file\n",
    "unproc_images_folder_2 = \"./preprocessing/imgsUnProcessed2\"  # Output folder to save the images\n",
    "if not os.path.exists(unproc_images_folder_2):\n",
    "    os.makedirs(unproc_images_folder_2)\n",
    "pdf_to_images(pdf_path2, unproc_images_folder_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0GKH7lTjtUP"
   },
   "source": [
    "### Splitting two sided scanned images into individual pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NxE9-mExPPQp",
    "outputId": "fc46dd53-e89b-4961-eb3e-7b20db7674d7"
   },
   "outputs": [],
   "source": [
    "#Segregating 2-sided pages into individual pages\n",
    "from utils import process_images\n",
    "\n",
    "unproc_images_folder_1 = \"./preprocessing/imgsUnProcessed1\"\n",
    "proc_images_folder_1 = \"./preprocessing/imgsForAllPages1\"\n",
    "if not os.path.exists(proc_images_folder_1):\n",
    "    os.makedirs(proc_images_folder_1)\n",
    "process_images(unproc_images_folder_1, proc_images_folder_1)\n",
    "\n",
    "\n",
    "unproc_images_folder_2 = \"./preprocessing/imgsUnProcessed2\"\n",
    "proc_images_folder_2 = \"./preprocessing/imgsForAllPages2\"\n",
    "if not os.path.exists(proc_images_folder_2):\n",
    "    os.makedirs(proc_images_folder_2)\n",
    "process_images(unproc_images_folder_2, proc_images_folder_2)\n",
    "\n",
    "print(\"Image processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHh0TrCmH-gu"
   },
   "source": [
    "### Downloading the CRAFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOVaNBR1PPOf",
    "outputId": "a061dfc2-d321-4962-99ee-e37debd86b30"
   },
   "outputs": [],
   "source": [
    "# Check if the folder already exists\n",
    "if not os.path.exists('CRAFT_Model'):\n",
    "    # If it doesn't exist, clone the repository\n",
    "    !git clone 'https://github.com/lochan13/CRAFT.git'\n",
    "else:\n",
    "    print(f\"The repository already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DegjoeDpj6XO"
   },
   "source": [
    "# Text Detection\n",
    "### Extracting words from a scanned text page image we are using the [CRAFT Model](https://github.com/clovaai/CRAFT-pytorch) for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WihUk3I9PO-_",
    "outputId": "0dc70474-9db4-4985-8375-359b81fdc0fb"
   },
   "outputs": [],
   "source": [
    "!python3 CRAFT/test.py --result_folder='./preprocessing/BoundBoxApplied1/' --test_folder=\"./preprocessing/imgsForAllPages1\" --trained_model='CRAFT/weights/craft_mlt_25k.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be1Zo-bq00IQ",
    "outputId": "304c27ce-21ef-4a42-992e-44124aaa78a7"
   },
   "outputs": [],
   "source": [
    "!python3 CRAFT/test.py --result_folder='./preprocessing/BoundBoxApplied2/' --test_folder=\"./preprocessing/imgsForAllPages2\" --trained_model='CRAFT/weights/craft_mlt_25k.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAN5mE9UlMtT"
   },
   "source": [
    "### The output of this model provides coordinates of the polygon enclosing the word. Using these coordinates one can draw a bounding box and crop word images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOkMKjs1QKTA"
   },
   "outputs": [],
   "source": [
    "#Sorting the BB based on the Spanish writing style\n",
    "from utils import sort_bounding_boxes\n",
    "\n",
    "bound_box_applied1 = './preprocessing/BoundBoxApplied1/'\n",
    "bound_box_sorted1 = \"./preprocessing/BoundBoxSorted1\"\n",
    "if not os.path.exists(bound_box_sorted1):\n",
    "    os.makedirs(bound_box_sorted1)\n",
    "sort_bounding_boxes(bound_box_applied1, bound_box_sorted1)\n",
    "\n",
    "bound_box_applied2 = './preprocessing/BoundBoxApplied2/'\n",
    "bound_box_sorted2 = \"./preprocessing/BoundBoxSorted2\"\n",
    "if not os.path.exists(bound_box_sorted2):\n",
    "    os.makedirs(bound_box_sorted2)\n",
    "sort_bounding_boxes(bound_box_applied2, bound_box_sorted2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmTRVvq3ooeE"
   },
   "source": [
    "# Generating Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zioZvAMFuDG"
   },
   "source": [
    "### This code is preparing test data by extracting individual words from scanned document pages, using pre-computed bounding box information. The extracted words are saved as separate image files, which can then be used for testing an OCR (Optical Character Recognition) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ud4k61img0JU",
    "outputId": "53f2c60b-7996-4e6a-cc0b-352f6d82b6bb"
   },
   "outputs": [],
   "source": [
    "from utils import count_files_in_folder, extract_bounding_boxes\n",
    "\n",
    "def apply_extraction_to_folder_for_test(image_folder, bounding_box_folder, output_folder, word, TRAIN_PAGES):\n",
    "    image_count = count_files_in_folder(image_folder, ['.png', '.jpeg', '.jpg'])\n",
    "\n",
    "    for image in range(TRAIN_PAGES, image_count):\n",
    "        image_filename = f'image_{image+1}.png'\n",
    "        image_base_name = f'image_{image+1}'\n",
    "        bounding_box_filename = f\"res_{image_base_name}_sorted.txt\"\n",
    "        bounding_box_path = os.path.join(bounding_box_folder, bounding_box_filename)\n",
    "\n",
    "        if os.path.exists(bounding_box_path):\n",
    "            image_path = os.path.join(image_folder, image_filename)\n",
    "            word = extract_bounding_boxes(image_path, bounding_box_path, output_folder, word)\n",
    "            print(image_base_name)\n",
    "        else:\n",
    "            print(f'Bounding box file for {image_filename} does not exist.')\n",
    "\n",
    "    return word\n",
    "\n",
    "# For testing_data1\n",
    "proc_images_folder_1 = './preprocessing/imgsForAllPages1'\n",
    "bound_box_sorted1 = './preprocessing/BoundBoxSorted1'\n",
    "testing_data1 = './testing_data1'\n",
    "word = apply_extraction_to_folder_for_test(proc_images_folder_1, bound_box_sorted1, testing_data1, 0, 25)\n",
    "\n",
    "# For testing_data2\n",
    "proc_images_folder_2 = './preprocessing/imgsForAllPages2'\n",
    "bound_box_sorted2 = './preprocessing/BoundBoxSorted2'\n",
    "testing_data2 = './testing_data2'\n",
    "apply_extraction_to_folder_for_test(proc_images_folder_2, bound_box_sorted2, testing_data2, word, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sO506CCMhKlx",
    "outputId": "10979dff-11f5-4dcf-fc8f-0c3edfae49cf"
   },
   "outputs": [],
   "source": [
    "from utils import pad_and_resize_images\n",
    "\n",
    "testing_data1 = './testing_data1'\n",
    "pad_and_resize_images(testing_data1)\n",
    "\n",
    "testing_data2= './testing_data2'\n",
    "pad_and_resize_images(testing_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc9nnjBqPygH"
   },
   "source": [
    "# Gathering Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RetFPbJBooeH"
   },
   "source": [
    "### Manually Corrected Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtXhuarYQE6L",
    "outputId": "5d68dc36-74ef-48a1-f976-0d9df1a0f136"
   },
   "outputs": [],
   "source": [
    "# Training data, corrrected manually\n",
    "!gdown 'https://drive.google.com/uc?id=1Gd9BWP0aixcSrs_vkDXJewd1sZEir8-m'\n",
    "!unzip outputCorrected.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K-venN_Ql47"
   },
   "outputs": [],
   "source": [
    "# Transferring the corrected dataset to the correct location for training\n",
    "!mv './outputChanged/output' './training_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dXEw95DooeI"
   },
   "source": [
    "# Working on Training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzgN9KHeooeI"
   },
   "source": [
    "### Padding small sized words for no distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhSxbmcIypEF",
    "outputId": "04e9afc5-23bc-4291-836f-1ba24bc61548",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding padding to the left and right of small sized words\n",
    "training_data = './training_data'\n",
    "pad_and_resize_images(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4Tcy3ACQtQF"
   },
   "source": [
    "### Image Augmentation by Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzLcax0oQysH",
    "outputId": "7dfcf69e-49ee-446c-daa7-69dc0152832e"
   },
   "outputs": [],
   "source": [
    "from utils import rotation_aug\n",
    "\n",
    "training_data = './training_data'\n",
    "rotation_aug(training_data)\n",
    "print(\"Image augmentation by Rotation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG6u8UP0QwI-"
   },
   "source": [
    "### Image augmentation by Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaX4S4ghUYNF",
    "outputId": "1c4e7964-e550-476b-d812-9c1498886094"
   },
   "outputs": [],
   "source": [
    "from utils import gaussian_noise_aug\n",
    "\n",
    "training_data = './training_data'\n",
    "gaussian_noise_aug(training_data)\n",
    "print(\"Image augmentation by Gaussian Noise completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8kNzkZpF2-l"
   },
   "source": [
    "### This part of code creates structured CSV files that can be used to organize and access the image data for future task, particularly for training and testing an OCR (Optical Character Recognition) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSv_WuGXQ0XX",
    "outputId": "bd6a4dc4-271f-4a6e-9041-3cbd3dc5258a"
   },
   "outputs": [],
   "source": [
    "from utils import create_csv_from_folder\n",
    "\n",
    "# Train data\n",
    "training_data = './training_data'\n",
    "train_csv_path = './training_data.csv'\n",
    "create_csv_from_folder(training_data, train_csv_path)\n",
    "\n",
    "# Test data\n",
    "test_data1 = \"./testing_data1\"\n",
    "test_csv_path1 = './testing_data1.csv'\n",
    "create_csv_from_folder(test_data1, test_csv_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiQ7X2YqGFD5"
   },
   "source": [
    "### Splitting train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQSk6DTyQ0Uw",
    "outputId": "09904d92-9f58-4f99-99a3-898575acf12d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAChcLjtH-g3"
   },
   "source": [
    "### create a new column IDENTITY that contain the label of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV3h0Y2CY2Iv"
   },
   "outputs": [],
   "source": [
    "df['IDENTITY'] = df['IDENTITY'].apply(lambda x: x.split('_')[0]).apply(lambda x: x.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1noyA0GyY2k_"
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(df.shape[0]*0.8)\n",
    "df_train = df.iloc[:TRAIN_SIZE]\n",
    "df_valid = df.iloc[TRAIN_SIZE+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "J47mQOmkRLzV",
    "outputId": "42819c60-6981-4b52-8ba8-758579138ec8"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv', index=False)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pJ0wd-eGRNBt",
    "outputId": "a3ad3007-94e9-4164-f75f-b656b58f4702",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_valid.to_csv('valid.csv', index=False)\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_5hMfPpGy27"
   },
   "source": [
    "### This code sets up configuration parameters and file paths for training, validating, and testing an OCR model, including image size, batch size, number of epochs, model name, callbacks, learning rate, random seeds, dataset file paths, data sizes, and prefetching settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0NtSnfPRM_D"
   },
   "outputs": [],
   "source": [
    "# Image Size\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "IMAGE_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# EPOCHS\n",
    "EPOCHS = 100\n",
    "\n",
    "# Model Name\n",
    "MODEL_NAME = 'SpanishOCR'\n",
    "\n",
    "# Callbacks\n",
    "CALLBACKS = [\n",
    "    callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    callbacks.ModelCheckpoint(filepath=MODEL_NAME + \".h5\", save_best_only=True)  # .h5 extension removed\n",
    "]\n",
    "\n",
    "# Learning Rate\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Random Seed\n",
    "np.random.seed(2569)\n",
    "tf.random.set_seed(2569)\n",
    "\n",
    "# File Paths ( replace 1 by 2,3 ... so as to test other folders)\n",
    "train_csv_path = './train.csv'\n",
    "valid_csv_path = './valid.csv'\n",
    "test_csv_path = './testing_data1.csv'\n",
    "\n",
    "train_image_dir = './training_data'\n",
    "valid_image_dir = './training_data'\n",
    "test_image_dir = './testing_data1'\n",
    "\n",
    "# AUTOTUNE\n",
    "AUTOTUNE = tfd.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ts1RXwJRis2"
   },
   "outputs": [],
   "source": [
    "# Train CSV\n",
    "train_csv = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Validation CSV\n",
    "valid_csv = pd.read_csv(valid_csv_path)\n",
    "\n",
    "# Test CSV\n",
    "test_csv = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H41F-RMgRlVh"
   },
   "outputs": [],
   "source": [
    "# In order to convert int to string to prevent tf error\n",
    "test_csv['IDENTITY'] = test_csv['IDENTITY'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4Tl7svbRlTX",
    "outputId": "522efd0e-30df-4636-af23-d8dac89f4008"
   },
   "outputs": [],
   "source": [
    "labels = [str(word) for word in df['IDENTITY'].to_numpy()]\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY4Z3iClIi47"
   },
   "source": [
    "### This code extracts unique characters from the dataset labels, calculates the total number of unique characters, and prints both the count and the set of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSz5lGSzRlRH",
    "outputId": "a12ab13b-f2e1-4ce5-a429-605c181b20c3"
   },
   "outputs": [],
   "source": [
    "# Unique characters\n",
    "unique_chars = set(char for word in labels for char in word)\n",
    "n_classes = len(unique_chars)\n",
    "\n",
    "# Show\n",
    "print(f\"Total number of unique characters : {n_classes}\")\n",
    "print(f\"Unique Characters : \\n{unique_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEebLJQ1RiqS",
    "outputId": "ecf37b53-b95a-493d-9951-e14ce6798b05"
   },
   "outputs": [],
   "source": [
    "MAX_LABEL_LENGTH = max(map(len, labels))\n",
    "print(f\"Maximum length of a label : {MAX_LABEL_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLtGbcVnIqXZ"
   },
   "source": [
    "### This code updates the file paths in the FILENAME columns of the training, validation, and test CSV dataframes by prepending the respective image directory paths to each filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTn3oSgdR0H5"
   },
   "outputs": [],
   "source": [
    "train_csv['FILENAME'] = [train_image_dir + f\"/{filename}\" for filename in train_csv['FILENAME']]\n",
    "valid_csv['FILENAME'] = [valid_image_dir + f\"/{filename}\" for filename in valid_csv['FILENAME']]\n",
    "test_csv['FILENAME']  = [test_image_dir + f\"/{filename}\" for filename in test_csv['FILENAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DqB76qgvR0FV",
    "outputId": "dbf22efa-ee00-446b-b47e-f5379e654569"
   },
   "outputs": [],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0zthpxUJLYir",
    "outputId": "83776796-63fb-46f9-eb73-b7929fd81710"
   },
   "outputs": [],
   "source": [
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXMcq8G8Iv2r"
   },
   "source": [
    "### This code creates two dictionaries: one that maps unique characters to numeric values using StringLookup, and another that reverses this mapping, converting numeric values back to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x-XQVLnR6LZ"
   },
   "outputs": [],
   "source": [
    "# Character to numeric value dictionary\n",
    "char_to_num = layers.StringLookup(\n",
    "    vocabulary = list(unique_chars),\n",
    "    mask_token = None\n",
    ")\n",
    "\n",
    "# Reverse dictionary\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary = char_to_num.get_vocabulary(),\n",
    "    mask_token = None,\n",
    "    invert = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nyyhlhxI0hm"
   },
   "source": [
    "### This function loads an image from a given path, decodes it as a JPEG, converts it to a tensor, resizes and normalizes it, and then returns the processed image as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obU7Gl2ER6ID"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path : str):\n",
    "    '''\n",
    "    This function loads and preprocesses images. It first receives the image path, which is used to\n",
    "    decode the image as a JPEG using TensorFlow. Then, it converts the image to a tensor and applies\n",
    "    two processing functions: resizing and normalization. The processed image is then returned by\n",
    "    the function.\n",
    "\n",
    "    Argument :\n",
    "        image_path : The path of the image file to be loaded.\n",
    "\n",
    "    Return:\n",
    "        image : The loaded image as a tensor.\n",
    "    '''\n",
    "\n",
    "    # Read the Image\n",
    "    image = tf.io.read_file(image_path)\n",
    "\n",
    "    # Decode the image\n",
    "    decoded_image = tf.image.decode_jpeg(contents = image, channels = 1)\n",
    "\n",
    "    # Convert image data type.\n",
    "    cnvt_image = tf.image.convert_image_dtype(image = decoded_image, dtype = tf.float32)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = tf.image.resize(images = cnvt_image, size = (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "    # Transpose\n",
    "    image = tf.transpose(resized_image, perm = [1, 0, 2])\n",
    "\n",
    "    # Convert image to a tensor.\n",
    "    image = tf.cast(image, dtype = tf.float32)\n",
    "\n",
    "    # Return loaded image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvz55g8qI9j4"
   },
   "source": [
    "### This function loads and preprocesses an image, converts a given label string into a sequence of numeric values, pads the numeric sequence to a fixed length, and returns a dictionary containing the processed image tensor and the label tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbeqmnrCR-c3"
   },
   "outputs": [],
   "source": [
    "def encode_single_sample(image_path : str, label : str):\n",
    "\n",
    "    '''\n",
    "    The function takes an image path and label as input and returns a dictionary containing the processed image tensor and the label tensor.\n",
    "    First, it loads the image using the load_image function, which decodes and resizes the image to a specific size. Then it converts the given\n",
    "    label string into a sequence of Unicode characters using the unicode_split function. Next, it uses the char_to_num layer to convert each\n",
    "    character in the label to a numerical representation. It pads the numerical representation with a special class (n_classes)\n",
    "    to ensure that all labels have the same length (MAX_LABEL_LENGTH). Finally, it returns a dictionary containing the processed image tensor\n",
    "    and the label tensor.\n",
    "\n",
    "    Arguments :\n",
    "        image_path : The location of the image file.\n",
    "        label      : The text to present in the image.\n",
    "\n",
    "    Returns:\n",
    "        dict : A dictionary containing the processed image and label.\n",
    "    '''\n",
    "\n",
    "    # Get the image\n",
    "    image = load_image(image_path)\n",
    "\n",
    "    # Convert the label into characters\n",
    "    chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "\n",
    "    # Convert the characters into vectors\n",
    "    vecs = char_to_num(chars)\n",
    "\n",
    "    # Pad label\n",
    "    pad_size = MAX_LABEL_LENGTH - tf.shape(vecs)[0]\n",
    "    vecs = tf.pad(vecs, paddings = [[0, pad_size]], constant_values=n_classes+1)\n",
    "\n",
    "    return {'image':image, 'label':vecs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sK_BmZRJBpW"
   },
   "source": [
    "### This code splits the combined dataset into training and validation subsets, shuffles the data, and then creates batched and prefetched TensorFlow datasets for efficient data loading during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UjokY-uR-aZ"
   },
   "outputs": [],
   "source": [
    "# Further split the training dataset into training and validation subsets\n",
    "train_size = int(0.8*len(df))  # 80% of the training data for training\n",
    "val_size = len(df) - train_size  # Remaining 20% for validation\n",
    "\n",
    "# Splitting the training dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(df['FILENAME'].to_list()), np.array(df['IDENTITY'].to_list()))\n",
    ").shuffle(train_size)\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = train_ds.take(train_size).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Create validation dataset\n",
    "valid_ds = train_ds.skip(train_size).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fdeiMX4JHK0"
   },
   "source": [
    "### This code creates TensorFlow datasets for training, validation, and testing by loading image paths and labels, shuffling the training data, applying the encode_single_sample function to preprocess the images and labels, batching the data, and using prefetching for optimized data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqoC4msJR-Yn"
   },
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(train_csv['FILENAME'].to_list()), np.array(train_csv['IDENTITY'].to_list()))\n",
    ").shuffle(train_size).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# # Validation data\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(valid_csv['FILENAME'].to_list()), np.array(valid_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# # Testing data.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(test_csv['FILENAME'].to_list()), np.array(test_csv['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTb_GZiCR-Wt",
    "outputId": "4b7782ad-660f-4995-94b6-fbf88bde1ea7"
   },
   "outputs": [],
   "source": [
    "print(f\"Training Data Size   : {tf.data.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Validation Data Size : {tf.data.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Testing Data Size    : {tf.data.Dataset.cardinality(test_ds).numpy() * BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXaMyf0AJOpd"
   },
   "source": [
    "### Resizes all images in the input folder to a specified size and saves them as PNG files in the output folder, ensuring the output directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3klqXm2R-UX"
   },
   "outputs": [],
   "source": [
    "# Set the new size in pixels (width, height) according to your choice\n",
    "def resize_images_in_folder(input_folder, new_size=(200,50)):\n",
    "    # Loop through all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # Open the image\n",
    "        with Image.open(os.path.join(input_folder, filename)) as img:\n",
    "            # Resize the image\n",
    "            resized_img = img.resize(new_size)\n",
    "            # Save the resized image to the output folder\n",
    "            output_filename = os.path.splitext(filename)[0] + '.png'  # Ensure output format is PNG\n",
    "            resized_img.save(os.path.join(input_folder, output_filename))\n",
    "\n",
    "training_image_dir = \"./training_data\"\n",
    "resize_images_in_folder(training_image_dir)\n",
    "\n",
    "test_image_dir = \"./testing_data1\"\n",
    "resize_images_in_folder(test_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laMe-ompJT09"
   },
   "source": [
    "### Displays a grid of images with their true labels, and optionally their predicted labels if a model is provided, using Matplotlib for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7VdlFq3SslC"
   },
   "outputs": [],
   "source": [
    "def show_images(data, GRID=[4,4], FIGSIZE=(25, 8), cmap='binary_r', model=None, decode_pred=None):\n",
    "\n",
    "    # Plotting configurations\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "    n_rows, n_cols = GRID\n",
    "\n",
    "    # Loading Data\n",
    "    data = next(iter(data))\n",
    "    images, labels = data['image'], data['label']\n",
    "\n",
    "    # Iterate over the data\n",
    "    for index, (image, label) in enumerate(zip(images, labels)):\n",
    "\n",
    "        # Label processing\n",
    "        text_label = num_to_char(label)\n",
    "        text_label = tf.strings.reduce_join(text_label).numpy().decode('UTF-8')\n",
    "        text_label = text_label.replace(\"[UNK]\", \" \").strip()\n",
    "\n",
    "        # Create a sub plot\n",
    "        plt.subplot(n_rows, n_cols, index+1)\n",
    "        plt.imshow(tf.transpose(image, perm=[1,0,2]), cmap=cmap)\n",
    "        plt.axis('off')\n",
    "\n",
    "        if model is not None and decode_pred is not None:\n",
    "            # Make prediction\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "            pred = decode_pred(pred)[0]\n",
    "            title = f\"True : {text_label}\\nPred : {pred}\"\n",
    "            plt.title(title)\n",
    "        else:\n",
    "            # add title\n",
    "            plt.title(text_label)\n",
    "\n",
    "    # Show the final plot\n",
    "    cls()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "DdMwK2VFSsie",
    "outputId": "26dd524c-0f77-46c1-f29b-37bda8e662f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_images(data=train_ds, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "y4ixapDbSB1e",
    "outputId": "900054d3-c242-4f79-9816-3b5aaaa08675"
   },
   "outputs": [],
   "source": [
    "show_images(data=test_ds, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqhYEXvjS1FH"
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFfHb05PJaJy"
   },
   "source": [
    "### Defines a custom Keras layer that computes and adds the Connectionist Temporal Classification (CTC) loss to the model for training sequence-to-sequence prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghs9ZfWjTKhF"
   },
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype='int64')\n",
    "        input_len = tf.cast(tf.shape(y_pred)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        label_len = tf.cast(tf.shape(y_true)[1], dtype='int64') * tf.ones(shape=(batch_len, 1), dtype='int64')\n",
    "        loss = self.loss_fn(y_true, y_pred, input_len, label_len)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkpygmE2JgDq"
   },
   "source": [
    "### Decodes the predicted labels from the OCR model, converting numeric values back to characters and removing unknown tokens to produce the final text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UP5HMpYSxPD"
   },
   "outputs": [],
   "source": [
    "def decode_pred(pred_label):\n",
    "\n",
    "    '''\n",
    "    The decode_pred function is used to decode the predicted labels generated by the OCR model.\n",
    "    It takes a matrix of predicted labels as input, where each time step represents the probability\n",
    "    for each character. The function uses CTC decoding to decode the numeric labels back into their\n",
    "    character values. The function also removes any unknown tokens and returns the decoded texts as a\n",
    "    list of strings. The function utilizes the num_to_char function to map numeric values back to their\n",
    "    corresponding characters. Overall, the function is an essential step in the OCR process, as it allows\n",
    "    us to obtain the final text output from the model's predictions.\n",
    "\n",
    "    Argument :\n",
    "        pred_label : These are the model predictions which are needed to be decoded.\n",
    "\n",
    "    Return:\n",
    "        filtered_text : This is the list of all the decoded and processed predictions.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Input length\n",
    "    input_len = np.ones(shape=pred_label.shape[0]) * pred_label.shape[1]\n",
    "\n",
    "    # CTC decode\n",
    "    decode = keras.backend.ctc_decode(pred_label, input_length=input_len, greedy=True)[0][0][:,:MAX_LABEL_LENGTH]\n",
    "\n",
    "    # Converting numerics back to their character values\n",
    "    chars = num_to_char(decode)\n",
    "\n",
    "    # Join all the characters\n",
    "    texts = [tf.strings.reduce_join(inputs=char).numpy().decode('UTF-8') for char in chars]\n",
    "\n",
    "    # Remove the unknown token\n",
    "    filtered_texts = [text.replace('[UNK]', \" \").strip() for text in texts]\n",
    "\n",
    "    return filtered_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD69WJr3Jmp_"
   },
   "source": [
    "### This code defines a deep learning model for optical character recognition (OCR) using a combination of convolutional neural networks (CNNs) for feature extraction and bidirectional long short-term memory (LSTM) networks for sequence learning, concluding with a custom CTC loss layer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNiR6lxySxM1"
   },
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "input_images = layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), name=\"image\")\n",
    "\n",
    "# Labels : These are added for the training purpose.\n",
    "target_labels = layers.Input(shape=(None, ), name=\"label\")\n",
    "\n",
    "# CNN Network\n",
    "x = layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal'\n",
    ")(input_images)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal'\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal'\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv2D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal'\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "\n",
    "# Encoding Space\n",
    "encoding = layers.Reshape(target_shape=((IMG_WIDTH//4), (IMG_HEIGHT//4)*128))(x)\n",
    "encoding = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(encoding)\n",
    "encoding = layers.Dropout(0.5)(encoding)\n",
    "\n",
    "# RNN Network\n",
    "x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.5))(encoding)\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.5))(x)\n",
    "\n",
    "# Output Layer\n",
    "output = layers.Dense(len(char_to_num.get_vocabulary())+1, activation='softmax')(x)\n",
    "\n",
    "# CTC Layer\n",
    "ctc_layer = CTCLayer()(target_labels, output)\n",
    "\n",
    "# Model\n",
    "ocr_model = keras.Model(\n",
    "    inputs=[input_images, target_labels],\n",
    "    outputs=[ctc_layer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVpPePCeZilm",
    "outputId": "28a596d4-481e-47ab-d8fb-c6e961217b46"
   },
   "outputs": [],
   "source": [
    "plot_model(ocr_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsPOHXQzJwze"
   },
   "source": [
    "### Compiles the OCR model with the Adam optimizer and trains it on the training dataset with validation, using early stopping and model checkpoint callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCpXgTIwZ5Gb",
    "outputId": "c6cd6f9a-598f-4c3f-9c77-a8d1198bf4e5"
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "ocr_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = ocr_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        lr_scheduler\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Learning Curve\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"CTC Loss Score\")\n",
    "plt.title(\"Learning Curve\", fontsize=15)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foKABcSzSxKh"
   },
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "ocr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-IBwnQ9J1qT"
   },
   "source": [
    "### Creates an inference model from the trained OCR model for making predictions and prints the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4zw-r86TOJJ"
   },
   "outputs": [],
   "source": [
    "inference_model = keras.Model(\n",
    "    inputs=ocr_model.input[0],  # Use only the first input if multiple\n",
    "    outputs=ocr_model.get_layer(name=\"dense_1\").output  # Adjust output layer name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA9RADpiTRJl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decode_pred(inference_model.predict(test_ds))[:10])   #convert to test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XigJbZ3M0g2T"
   },
   "outputs": [],
   "source": [
    "show_images(data=train_ds, model=inference_model, decode_pred=decode_pred, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GRuaQzmTTSu"
   },
   "outputs": [],
   "source": [
    "show_images(data=valid_ds, model=inference_model, decode_pred=decode_pred, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5HSCHRgoMl7"
   },
   "outputs": [],
   "source": [
    "show_images(data=test_ds, model=inference_model, decode_pred=decode_pred, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0xyr6_NPvxO"
   },
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koSw6ms2PyTy"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "ocr_model.save('ocr_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUbh-x9JJ651"
   },
   "source": [
    "### Generates and decodes predictions from the inference model on the test dataset, printing the results as it is written in the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hKcj5c6L7-8"
   },
   "outputs": [],
   "source": [
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40Clj60fod3K"
   },
   "outputs": [],
   "source": [
    "df_infer = test_csv\n",
    "\n",
    "# Step 1: Sort the dataframe based on values before ';'\n",
    "df_infer['before_semicolon'] = df_infer['IDENTITY'].apply(lambda x: int(x.split(';')[0]))\n",
    "df_infer['after_semicolon'] = df_infer['IDENTITY'].apply(lambda x: int(x.split(';')[1]))\n",
    "sorted_df = df_infer.sort_values(['before_semicolon']).reset_index(drop=True)\n",
    "sorted_df.drop(columns=['before_semicolon', 'after_semicolon'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpHNkckvp4SJ"
   },
   "outputs": [],
   "source": [
    "sorted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-rDzsu6oWlL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAqdNF7gx3LG"
   },
   "outputs": [],
   "source": [
    "sorted_df['IDENTITY'] = sorted_df['IDENTITY'].astype(str)\n",
    "\n",
    "sorted_dfs = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(sorted_df['FILENAME'].to_list()), np.array(sorted_df['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "decoded_predictions = decode_pred(inference_model.predict(sorted_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZVoRRp5NLVF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQuEglDPmJRc"
   },
   "outputs": [],
   "source": [
    "pred = sorted_df['IDENTITY'].tolist()\n",
    "\n",
    "# Print decoded predictions with a new line when there's a change in the number after ';'\n",
    "current_group = None\n",
    "i=0\n",
    "for prediction in pred:\n",
    "    before, after = map(int, prediction.split(';'))\n",
    "\n",
    "    if current_group is None:\n",
    "        current_group = after\n",
    "\n",
    "    if after != current_group:\n",
    "        print()  # Start a new line for the new group\n",
    "        current_group = after\n",
    "\n",
    "    print(decoded_predictions[i], end=' ')\n",
    "    i+=1\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCnQjmi_0G0k"
   },
   "source": [
    "# Saving pedictions as .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbReNuf40Gis"
   },
   "outputs": [],
   "source": [
    "formatted_output = []\n",
    "\n",
    "current_group = None\n",
    "i = 0\n",
    "for prediction in pred:\n",
    "    before, after = map(int, prediction.split(';'))\n",
    "\n",
    "    if current_group is None:\n",
    "        current_group = after\n",
    "\n",
    "    if after != current_group:\n",
    "        formatted_output.append('\\n')  # Start a new line for the new group\n",
    "        current_group = after\n",
    "\n",
    "    formatted_output.append(decoded_predictions[i] + ' ')\n",
    "    i += 1\n",
    "\n",
    "formatted_output.append('\\n')  # Final new line\n",
    "\n",
    "with open('formatted_predictions.txt', 'w') as file:\n",
    "    file.writelines(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCPZokCozGH_"
   },
   "outputs": [],
   "source": [
    "predictions = decode_pred(inference_model.predict(valid_ds))\n",
    "true = df_valid['IDENTITY'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juQ2jNf_xuoA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_cer(predicted, ground_truth):\n",
    "    # Calculate the Levenshtein distance between the predicted and ground truth\n",
    "    matcher = SequenceMatcher(None, ground_truth, predicted)\n",
    "    num_edits = sum(triple[-1] for triple in matcher.get_opcodes() if triple[0] != 'equal')\n",
    "\n",
    "    # CER is the number of edits divided by the total number of characters in the ground truth\n",
    "    return num_edits / len(ground_truth) if len(ground_truth) > 0 else 1.0\n",
    "\n",
    "def calculate_accuracy(true_list, predictions_list):\n",
    "    correct_count = sum([pred == gt for pred, gt in zip(predictions_list, true_list)])\n",
    "    return correct_count / len(true_list)\n",
    "\n",
    "# Calculate CER for each word and the overall Accuracy\n",
    "cer_list = [calculate_cer(pred, gt) for pred, gt in zip(predictions, true)]\n",
    "cer = np.mean(cer_list)\n",
    "\n",
    "accuracy = calculate_accuracy(true, predictions)\n",
    "\n",
    "print(f\"Character Error Rate (CER): {cer}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCJXoPUBLjcx"
   },
   "outputs": [],
   "source": [
    "sorted_df = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array(sorted_df['FILENAME'].to_list()), np.array(sorted_df['IDENTITY'].to_list()))\n",
    ").map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "show_images(data=sorted_df, model=inference_model, decode_pred=decode_pred, cmap='binary')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
